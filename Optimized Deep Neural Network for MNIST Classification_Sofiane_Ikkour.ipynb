{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Optimized Deep Neural Network for MNIST Classification_Sofiane_Ikkour.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyO8Qvu4toj2WwKX62cQcnuc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"88196cb8790c4e7ea475083c6a251c71":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_3e557f3389c14560a01b8bee0e631c80","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_7f285d4e7ed34bd5a24735e3ca548ecf","IPY_MODEL_477a326230d14e0b9090cff1fb7e9be5","IPY_MODEL_6872de1942fc465ab9ab495bf53d21e8"]}},"3e557f3389c14560a01b8bee0e631c80":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7f285d4e7ed34bd5a24735e3ca548ecf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_319e8e93d1b34dc48e928f2f875e7b71","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Dl Completed...: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_dd19a4f8a8c44c1ea33bcc2cb565a521"}},"477a326230d14e0b9090cff1fb7e9be5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_0e38abadaa004af8aa838550d6ab35ac","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":4,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":4,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_9ad677537b95424d8c4d302930094661"}},"6872de1942fc465ab9ab495bf53d21e8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_93f981f3690546c69211ed82301ccaff","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 4/4 [00:00&lt;00:00,  8.09 file/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1727d5538ebd47f49a7eaf0961af8e23"}},"319e8e93d1b34dc48e928f2f875e7b71":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"dd19a4f8a8c44c1ea33bcc2cb565a521":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0e38abadaa004af8aa838550d6ab35ac":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"9ad677537b95424d8c4d302930094661":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"93f981f3690546c69211ed82301ccaff":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"1727d5538ebd47f49a7eaf0961af8e23":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","source":["# **Optimized Deep Neural Network for MNIST Classification**"],"metadata":{"id":"1amlGbkJlJPF"}},{"cell_type":"markdown","source":["The goal of this work is to write a program capable of detecting which digit is written based on the MNIST dataset. The MNIST dataset refers to handwritten digit recognition and provides 70000 images (28x28 pixels) of handwritten digits (1 digit per image). This classification is a problem with 10 classes since we have 10 digits (0,1,2,3,4,5,6,7,8,9). \n","\n","After building the network with only two hidden layers and 50 units per hidden layer (see \"Deep Neural Network for MNIST Classification_Sofiane_Ikkour\" code), I tuned some hyperparameters and tried several combinations. The final tuning ended up improving the accuracy from 96.92% to ~98%.\n","\n","Below are some details about the different tries it took to reach the final result:\n","\n","- The width of the algorithm: the size of both hidden layers was changed to 200.\n","- The depth of the algorithm: the number of the hidden layers was changed to 4.\n","- Activation functions: the sigmoid activation function was applied to every hidden layer. The softmax activation function was applied to the output layer.  \n","- Learning Rate: 0.0001, 0.02.\n","- Batch size adjusted: 1 (that's the SGD).\n","- Number of epochs: 5, 8, 10.\n","\n","The final combination I tried gave an accuracy of about 98% (see Hyperparamter Optimization section at the end of this notebook).\n","\n","Although this is a remarkable result, there is still room for improvement and an accuracy of 99% is possible with the right combination of hyperparameters.\n","\n","\n","\n","\n","\n","Note: this code was written on Google Colab."],"metadata":{"id":"Mxoia0u6lbNq"}},{"cell_type":"markdown","source":["**Import the relevant packages**"],"metadata":{"id":"EXnBhIdvoZna"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ydZcSjr7lIqH"},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","import tensorflow_datasets as tfds # this package provides access to the MNIST dataset"]},{"cell_type":"markdown","source":["**Data preprocessing**"],"metadata":{"id":"Qtdx4u8nQBSg"}},{"cell_type":"code","source":["# Load the MNIST dataset\n","mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)\n","\n","# the argument name helps us specify the name of the dataset we want to load\n","# the argument with_info returns a tuple that stores information about the dataset\n","# the argument as_supervised, when True, returns a 2-tuple structure (input, target), when False, it returns a dictionary containg all the features\n","# the argument split splits the dataset into a train and test sets\n","\n","# now let's extract the train and test set separately\n","mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']"],"metadata":{"id":"uFaCWRD0o72I","colab":{"base_uri":"https://localhost:8080/","height":188,"referenced_widgets":["88196cb8790c4e7ea475083c6a251c71","3e557f3389c14560a01b8bee0e631c80","7f285d4e7ed34bd5a24735e3ca548ecf","477a326230d14e0b9090cff1fb7e9be5","6872de1942fc465ab9ab495bf53d21e8","319e8e93d1b34dc48e928f2f875e7b71","dd19a4f8a8c44c1ea33bcc2cb565a521","0e38abadaa004af8aa838550d6ab35ac","9ad677537b95424d8c4d302930094661","93f981f3690546c69211ed82301ccaff","1727d5538ebd47f49a7eaf0961af8e23"]},"executionInfo":{"status":"ok","timestamp":1642376860844,"user_tz":-60,"elapsed":1849,"user":{"displayName":"Sofiane Ikkour","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12163968403439377340"}},"outputId":"3974c80e-b4e7-431c-c118-8a0013619b6b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1mDownloading and preparing dataset mnist/3.0.1 (download: 11.06 MiB, generated: 21.00 MiB, total: 32.06 MiB) to /root/tensorflow_datasets/mnist/3.0.1...\u001b[0m\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:Dataset mnist is hosted on GCS. It will automatically be downloaded to your\n","local data directory. If you'd instead prefer to read directly from our public\n","GCS bucket (recommended if you're running on GCP), you can instead pass\n","`try_gcs=True` to `tfds.load` or set `data_dir=gs://tfds-data/datasets`.\n","\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"88196cb8790c4e7ea475083c6a251c71","version_minor":0,"version_major":2},"text/plain":["Dl Completed...:   0%|          | 0/4 [00:00<?, ? file/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","\u001b[1mDataset mnist downloaded and prepared to /root/tensorflow_datasets/mnist/3.0.1. Subsequent calls will reuse this data.\u001b[0m\n"]}]},{"cell_type":"code","source":["# print the mnist_train set to see what it looks like\n","mnist_train"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5ZtmND4PQQeq","executionInfo":{"status":"ok","timestamp":1642376933141,"user_tz":-60,"elapsed":269,"user":{"displayName":"Sofiane Ikkour","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12163968403439377340"}},"outputId":"5f070c40-30d6-453a-bd73-14c60b8e9892"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<PrefetchDataset shapes: ((28, 28, 1), ()), types: (tf.uint8, tf.int64)>"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["# tensorflow has a training and test datasets by default\n","# however it doesn't have a validation set, so we need to do the split ourselves\n","# first let's see what the variable mnist_info looks like\n","mnist_info"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QUZA2Yc938D4","executionInfo":{"status":"ok","timestamp":1642376934324,"user_tz":-60,"elapsed":13,"user":{"displayName":"Sofiane Ikkour","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12163968403439377340"}},"outputId":"d537e876-20f2-476f-f118-bc04c162ac8b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tfds.core.DatasetInfo(\n","    name='mnist',\n","    version=3.0.1,\n","    description='The MNIST database of handwritten digits.',\n","    homepage='http://yann.lecun.com/exdb/mnist/',\n","    features=FeaturesDict({\n","        'image': Image(shape=(28, 28, 1), dtype=tf.uint8),\n","        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),\n","    }),\n","    total_num_examples=70000,\n","    splits={\n","        'test': 10000,\n","        'train': 60000,\n","    },\n","    supervised_keys=('image', 'label'),\n","    citation=\"\"\"@article{lecun2010mnist,\n","      title={MNIST handwritten digit database},\n","      author={LeCun, Yann and Cortes, Corinna and Burges, CJ},\n","      journal={ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist},\n","      volume={2},\n","      year={2010}\n","    }\"\"\",\n","    redistribution_info=,\n",")"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["# mnist_info is a variable that stores multiple items\n","# the items (some of them are dictionaries) needed to define a validation set are \"splits\" and \"total_num_examples\" \n","\n","# we're going to define the number of validation samples as % of the training samples.  \n","num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples # 10% of the samples extracted\n","# we need to convert this variable to an integer\n","num_validation_samples = tf.cast(num_validation_samples, tf.int64)\n","\n","# we also need to define a variable in which we can store the number of test samples\n","# this will avoid using the minst_inf.splits method to extract the test samples when we need to\n","num_test_samples = mnist_info.splits['test'].num_examples\n","num_test_samples = tf.cast(num_test_samples, tf.int64) # convert the variable to an integer\n","\n","# next step is to scale our data in order to have inputs between 0 and 1\n","# we write a function that accepts two inputs, an input image and a label\n","# the goal is to make the results more numerically stable as the input images will take values between 0 and 1\n","def scale(image, label):\n","  # first we convert the image to a float\n","  image = tf.cast(image, tf.float32)\n","  # image pixel intensities are comprised between 0 and 255 (256 shades of grey)\n","  # we divide each input by 255\n","  image /= 255.\n","  return image, label\n","\n","# use the method map to apply the function scale on the train and test sets\n","train_validation_scaled = mnist_train.map(scale)\n","test_scaled = mnist_test.map(scale)\n","\n","# once the data is scaled, we will define a buffer_size to shuffle it\n","buffer_size = 10000\n","# this buffer_size is a parameter we use when dealing with very large datasets\n","# in this case, the dataset cannot be shuffled at once as it can't be fit in memory\n","# so instead, tensorflow stores only samples at a time and shuffles them\n","\n","# use the shuffle method\n","train_validation_scaled_shuffled = train_validation_scaled.shuffle(buffer_size)\n","\n","# now that the train data is scaled and shuffled, we need to extract the validation data from it\n","# the number of validation samples is defined previously with num_validation_samples variable\n","# use the .take() method to extract that many samples\n","validation_data = train_validation_scaled_shuffled.take(num_validation_samples)\n","\n","# now we use the .skip() method to extract the rest of the data as train data\n","train_data = train_validation_scaled_shuffled.skip(num_validation_samples)\n","\n","# let's now define a batch size and batch the train data\n","batch_size = 150\n","train_data = train_data.batch(batch_size)\n","\n","# batch the validation data\n","validation_data = validation_data.batch(num_validation_samples)\n","\n","# batch the test data\n","test_data = test_scaled.batch(num_test_samples)\n","\n","# as_supervised is a 2-tuple structure\n","# so take the next batch\n","validation_inputs, validation_targets = next(iter(validation_data))"],"metadata":{"id":"rLu-L8z35yp5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Outline the model**"],"metadata":{"id":"ClNFrkKqqYDi"}},{"cell_type":"code","source":["# our goal is to build a neural network with an input layer, output layer and 2 hidden layers\n","input_size = 784 # image size is 28 x 28\n","output_size = 10 # 10 classes (digits)\n","hidden_layer_size = 500 # I chose the same size for all hidden layers\n","\n","# define the model\n","model = tf.keras.Sequential([tf.keras.layers.Flatten(input_shape=(28,28,1)),\n","                             tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # first hidden layer\n","                             tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # second hidden layer\n","                             tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # third hidden layer\n","                             tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # fourth hidden layer\n","                             tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # fifth hidden layer\n","                             tf.keras.layers.Dense(output_size, activation='softmax')]) # output layer\n","                             # the method Flatten reorders the input image (28,28,1) into a (784,) vector\n","                             # the method Dense implements: output = activation(dot(input,weight) + bias)"],"metadata":{"id":"vzx9kbuKqRXN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# define the optimizer, the loss function and the metric \n","model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n","# since the labels are not one-hot encoded (integers) I used sparse_cross_entropy as a loss function"],"metadata":{"id":"xh1bcDAV50Oe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Training**"],"metadata":{"id":"GP4MA55g-rrb"}},{"cell_type":"code","source":["# next step is to train our data\n","# we specify the train_data, the number of epochs and the validation data we created\n","model.fit(train_data, epochs=10, validation_data=(validation_inputs, validation_targets), verbose=2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j0IbG8j19d_R","executionInfo":{"status":"ok","timestamp":1642378605035,"user_tz":-60,"elapsed":182334,"user":{"displayName":"Sofiane Ikkour","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12163968403439377340"}},"outputId":"4d357caa-667c-4849-afdc-414fe16fe41a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","360/360 - 15s - loss: 0.2450 - accuracy: 0.9251 - val_loss: 0.1122 - val_accuracy: 0.9665 - 15s/epoch - 42ms/step\n","Epoch 2/10\n","360/360 - 14s - loss: 0.0971 - accuracy: 0.9713 - val_loss: 0.0820 - val_accuracy: 0.9765 - 14s/epoch - 40ms/step\n","Epoch 3/10\n","360/360 - 15s - loss: 0.0711 - accuracy: 0.9785 - val_loss: 0.0611 - val_accuracy: 0.9818 - 15s/epoch - 41ms/step\n","Epoch 4/10\n","360/360 - 15s - loss: 0.0557 - accuracy: 0.9821 - val_loss: 0.0524 - val_accuracy: 0.9847 - 15s/epoch - 41ms/step\n","Epoch 5/10\n","360/360 - 15s - loss: 0.0438 - accuracy: 0.9864 - val_loss: 0.0592 - val_accuracy: 0.9843 - 15s/epoch - 41ms/step\n","Epoch 6/10\n","360/360 - 15s - loss: 0.0362 - accuracy: 0.9889 - val_loss: 0.0448 - val_accuracy: 0.9877 - 15s/epoch - 42ms/step\n","Epoch 7/10\n","360/360 - 15s - loss: 0.0349 - accuracy: 0.9894 - val_loss: 0.0443 - val_accuracy: 0.9882 - 15s/epoch - 42ms/step\n","Epoch 8/10\n","360/360 - 15s - loss: 0.0292 - accuracy: 0.9912 - val_loss: 0.0573 - val_accuracy: 0.9858 - 15s/epoch - 41ms/step\n","Epoch 9/10\n","360/360 - 14s - loss: 0.0263 - accuracy: 0.9922 - val_loss: 0.0429 - val_accuracy: 0.9865 - 14s/epoch - 39ms/step\n","Epoch 10/10\n","360/360 - 15s - loss: 0.0237 - accuracy: 0.9925 - val_loss: 0.0359 - val_accuracy: 0.9902 - 15s/epoch - 40ms/step\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7fda27724a90>"]},"metadata":{},"execution_count":24}]},{"cell_type":"markdown","source":["**Testing**"],"metadata":{"id":"D5mHf4vRAeBb"}},{"cell_type":"code","source":["# after training our model, we need to test it on the test data \n","test_loss, test_accuracy = model.evaluate(test_data) "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wOkMussyAcZ9","executionInfo":{"status":"ok","timestamp":1642378613238,"user_tz":-60,"elapsed":1386,"user":{"displayName":"Sofiane Ikkour","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12163968403439377340"}},"outputId":"168151c0-d93d-45bd-85ad-68060c8f6256"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 1s 836ms/step - loss: 0.1038 - accuracy: 0.9767\n"]}]},{"cell_type":"code","source":["print('The test loss of our model is', round(test_loss*100,2),'%')\n","print('The test accuracy of our model is', round(test_accuracy*100,2),'%')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jbQNIjm-DfTu","executionInfo":{"status":"ok","timestamp":1642378618736,"user_tz":-60,"elapsed":6,"user":{"displayName":"Sofiane Ikkour","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12163968403439377340"}},"outputId":"f1ac3e61-3fe4-4358-fefb-73d70596f1f4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The test loss of our model is 10.38 %\n","The test accuracy of our model is 97.67 %\n"]}]},{"cell_type":"markdown","source":["**Hyperparameter optimiztion**"],"metadata":{"id":"63w2Mf6mETCz"}},{"cell_type":"markdown","source":["Many hyperparameter adjustements have been applied in order to improve the accuracy of the model. \n","I chose the following hyperparameters:  \n","- batch_size = 150  \n","- hidden units = 500\n","- hidden layers = 5\n","- Number of epochs = 10\n","\n","The accuracy improved from 96.92% to ~98%.\n","This result is remarkably good given the number of hidden layers and the number of units used. This can be improved even further."],"metadata":{"id":"D9QXaZPUGruL"}}]}